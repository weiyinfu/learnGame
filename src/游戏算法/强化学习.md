
# 游戏AI
AlphaZero, Muzero, imitation learning, Pluribus  
从AlphaGo到AlphaStar，OpenAI  Five，到腾讯的绝艺，绝悟，到启元世界，超参数等初创公司的出现，深度强化学习带来一波全新的AI热潮。在这个阶段，笔者觉得游戏AI就是通过模仿学习及深度强化学习等技术通过大规模的分布式训练而得到的竞技AI，这种AI可以在MOBA，吃鸡及其他竞技类游戏中得到应用。

学术界工业界都开发了很多实现大规模深度强化学习的框架和理论：
- Ray/Rllib
- Fiber
- OpenAI Rapid
- Google Seed RL

# 从零开始构建深度强化学习的知识体系
显然，要成为一个合格的深度强化学习研发人员，需要对整个知识体系有完全的了解，并且对其中的主要部分有深刻的理解和实践。

一般，我们分以下几步来构建知识体系：
Step 1：深度学习，掌握神经网络的各种基本模型
当前，对于构建深度强化学习的网络模型，主要会用到这些组件：
1. CNN
2. RNN/LSTM
3. Attention, Transformer, Pointer Network
4. Memory Network, DNC,...
5. Graph Network （目前还比较少）
   Step 2：深度强化学习算法
   这部分是深度强化学习的核心，目前主流包括：
6. DQN
7. Rainbow，Distributional Q Learning
8. IMPALA，UPGO
9. SAC
10. R2D2，R2D3，APE-X
11. PPO，APPO
    除此之外，深度强化学习算法理论还包括了：
12. Model-based Learning
13. MCTS (有些问题可以用到比如围棋）
14. Exploration
15. Imitation Learning
16. Multi-task Learning
17. Meta Reinforcement Learning
18. Hierarchical Reinforcement Learning
    这些额外的部分都是为了辅助核心的算法让agent学的更快更强。
    对于额外的部分，知道的越多当然越好了。
    Step 3：Large Scale Deep Reinforcement Learning 大规模深度强化学习
    这部分的核心是理解掌握大规模深度强化学习的框架构建，目前主流的可以通过以下几种去理解（有的是框架，有的是开源代码，有的是理论）：
19. IMPALA
20. Rapid
21. Fiber
22. Seed RL
23. Rllib
24. R2D3
    这部分工作一般需要有研究MLsystem的童鞋配合，要不然搭不起来。目前K8s是比较好的选择，比如Fiber就完美支持K8s，OpenAI的集群也都是基于K8s搭建。
    Step 4： Meta Controller/Population-based Learning/Self-Play
    只有有了self-play，深度强化学习才能展现其魅力，这部分包括了：
25. AlphaGo self-play
26. AutoCurricula
27. Domain Randomization
28. AlphaStar League
29. Reward Shaping, Meta-learned Reward Shaping
30. Meta Gradient, Self-tuning reinforcement Learning
31. AI-generated Envs，Open-Endedness, Quality Diversity
    总的来说，这部分是在Meta层面去控制Agent的训练。
    前面三个step只能保证你可以训起来，能把Agent训成什么样，这部分是至关重要的。

